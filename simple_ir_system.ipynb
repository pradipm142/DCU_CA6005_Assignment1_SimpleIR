{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1bWFaAorLGWqI-zg_lR5aW_3mlb84rXEx","authorship_tag":"ABX9TyPKR+04VGO7qgs5cwAgnIop"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["**Pre-Requisites:**\n","\n","\n","*   Create a new folder, named \"DCU_IR_PROJECT\" in your Google Drive account.\n","*   Download, unzip, and load the Cranfield Collection files (cran.all.1400, cran.query, cranqrel, and cranqrel.readme) from https://github.com/terrierteam/jtreceval and load them in the above folder.\n","\n","\n","*   Run the below command to mount a google drive using your google account and giving the required access using the prompts.\n","\n","*   Update colab notebook settings to use GPU/TPU for faster processing\n","\n","\n","\n"],"metadata":{"id":"J66HDNdQtIND"}},{"cell_type":"code","source":["# Mount Google Drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lk852UvIbsyj","executionInfo":{"status":"ok","timestamp":1690500310279,"user_tz":-60,"elapsed":11424,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}},"outputId":"89ce5f45-235f-4335-8f77-00869f8e768f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   Install pytrec_eval evaluation module. It is a Python interface to TREC's evaluation tool which we are using for this project.\n","\n"],"metadata":{"id":"sp7F9Gotu9ej"}},{"cell_type":"code","source":["# Install pytrec_eval for the model evaluations\n","!pip install pytrec_eval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIRMjUOGtWj6","executionInfo":{"status":"ok","timestamp":1690500338679,"user_tz":-60,"elapsed":24207,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}},"outputId":"37e21d02-486e-4091-948f-45e5d3edae09"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytrec_eval\n","  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pytrec_eval\n","  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308109 sha256=6b16b1aa25fa6e965862136367b4bc36aeaf3a798d90b2e933e76049b7b22ca5\n","  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\n","Successfully built pytrec_eval\n","Installing collected packages: pytrec_eval\n","Successfully installed pytrec_eval-0.5\n"]}]},{"cell_type":"markdown","source":["\n","\n","*   Import the required python packages\n","\n"],"metadata":{"id":"gO2CjGN0vkvt"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"QIVb1WTlbjxi","executionInfo":{"status":"ok","timestamp":1690500354306,"user_tz":-60,"elapsed":5045,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"outputs":[],"source":["import os\n","import nltk\n","import math\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","from operator import itemgetter\n","import pytrec_eval\n","import pandas as pd"]},{"cell_type":"markdown","source":["\n","\n","*   Download NLTK \"StopWords\" module which includes a list of 40 stop words, including: \"a\", \"an\", \"the\", \"of\", \"in\", etc. The stopwords in nltk are the most common words in data. They are words that you do not want to use to describe the topic of your content.\n","*   Download \"Punkt\" tokenizer module which divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n","\n"],"metadata":{"id":"1CLi8nmevwzj"}},{"cell_type":"code","source":["# Download NLTK \"StopWords\" and \"Punkt\" packages\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwQOGgM7ehyw","executionInfo":{"status":"ok","timestamp":1690500362905,"user_tz":-60,"elapsed":713,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}},"outputId":"6cdf9251-4af2-4831-90ba-aaafa74079f0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["\n","\n","*   Declare various file paths for the programs to use/re-use, like path to the Cranfield files etc.\n","\n"],"metadata":{"id":"49Ql6SozwwDd"}},{"cell_type":"code","source":["# Set the file paths\n","data_path = '/content/drive/MyDrive/DCU_IR_PROJECT'\n","collection_file = os.path.join(data_path, 'cran.all.1400')\n","queries_file = os.path.join(data_path, 'cran.qry')\n","relevance_file = os.path.join(data_path, 'cranqrel')\n","original_document_path = os.path.join(data_path, 'originalDocument')\n","tokenized_document_path = os.path.join(data_path, 'tokenizedDocument')\n","output_dir = os.path.join(data_path, 'output')"],"metadata":{"id":"u9kI_DsFbuN2","executionInfo":{"status":"ok","timestamp":1690500370729,"user_tz":-60,"elapsed":175,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["*  Read the content of the Cranfield collection file and split the content into individual documents based on the delimiter \".I\".\n","*  For each document, extract the document ID and content and store each document in a dictionary, where the document ID is the key and the content is the value.\n","*  Create a folder named \"originalDocument\" (if it doesn't exist) to store individual document files with their content and write each document's content to a separate text file within the \"originalDocument\" folder, using the document ID as the filename.\n","*  Return the collection of documents as a dictionary."],"metadata":{"id":"yaRD1NYzxwZ8"}},{"cell_type":"code","source":["# Load the Cranfield collection\n","def load_cranfield_collection(collection_file):\n","    with open(collection_file, 'r', encoding='utf-8') as file:\n","        documents = file.read().split('.I')[1:]\n","\n","    collection = {}\n","    for i, doc in enumerate(documents, start=1):\n","        '''\n","        #Exiting the loop after 10 iterations\n","        if i > 10:  # Exit the loop after 10 iterations\n","            print(\"Exiting the loop after 10 iterations.\")\n","            break\n","        '''\n","        doc_id, content = doc.split('.T\\n')\n","        doc_id = doc_id.strip()\n","        content = content.strip()\n","        collection[doc_id] = content\n","\n","        # Create a new folder \"originalDocument\" if it doesn't exist\n","        if not os.path.exists(original_document_path):\n","            os.makedirs(original_document_path)\n","\n","        # Write the content to a new text file with doc_id as the file name\n","        doc_file = os.path.join(original_document_path, f'{doc_id}.txt')\n","        with open(doc_file, 'w', encoding='utf-8') as f:\n","            f.write(content)\n","\n","    return collection"],"metadata":{"id":"IabLja9cbzkr","executionInfo":{"status":"ok","timestamp":1690500378671,"user_tz":-60,"elapsed":235,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["This pre-processing function tokenizes the document contents, removes the stop words, performs stemming etc.\n","*  Tokenize: breaks the content into individual words or tokens.\n","*  Remove stop words: remove common English words like, \"the,\" \"is,\" \"and\" etc. that do not contribute much to the meaning of the text.\n","*  Stem the tokens: reduce each token to its base or root form using Porter stemming algorithm.\n","*  Create a folder, named \"tokenizedDocument\" (if it doesn't exist) and write the pre-processed tokens to a text file using the document Id as the filename.\n","*  Return the list of stemmed tokens.\n"],"metadata":{"id":"nl-FDBdYypbd"}},{"cell_type":"code","source":["# Function to pre-process the source documents\n","def preprocess_document(doc_id, doc_content):\n","    tokens = word_tokenize(doc_content.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    stemmer = PorterStemmer()\n","    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","\n","    # Create a new folder \"originalDocument\" if it doesn't exist\n","    if not os.path.exists(tokenized_document_path):\n","        os.makedirs(tokenized_document_path)\n","\n","    # Write the tokens to a new text file with doc_id as the file name\n","    doc_file = os.path.join(tokenized_document_path, f'{doc_id}.txt')\n","    with open(doc_file, 'w', encoding='utf-8') as f:\n","        f.write(' '.join(stemmed_tokens))\n","\n","    return stemmed_tokens"],"metadata":{"id":"Ju7FKEPGbzYp","executionInfo":{"status":"ok","timestamp":1690500388685,"user_tz":-60,"elapsed":183,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["*  Read the queries from the cranfield query files, split them into individual queries, and  then create a dictionary with query Ids as the keys and query content as the values and return the dictionary."],"metadata":{"id":"xwjDnSRF0S0B"}},{"cell_type":"code","source":["# Load the queries\n","def load_queries(queries_file):\n","    with open(queries_file, 'r', encoding='utf-8') as file:\n","        queries = file.read().split('.I')[1:]\n","\n","    queries_dict = {}\n","    for query in queries:\n","        query_id, content = query.split('.W\\n')\n","        queries_dict[query_id.strip()] = content.strip()\n","\n","    return queries_dict"],"metadata":{"id":"UK99xOxgbzPk","executionInfo":{"status":"ok","timestamp":1690500396494,"user_tz":-60,"elapsed":214,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["*  Calculate the Term Frequency (TF) of the each token by counting its occurrences and return a dictionary having the TF values for each token."],"metadata":{"id":"fue3NHGs0_gj"}},{"cell_type":"code","source":["# Function to calculate term frequency (TF)\n","def calculate_tf(query_tokens):\n","    tf = defaultdict(int)\n","    for token in query_tokens:\n","        tf[token] += 1\n","    return tf"],"metadata":{"id":"Bj3SeRnEbzEK","executionInfo":{"status":"ok","timestamp":1690500403135,"user_tz":-60,"elapsed":198,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["*  Take the inverted index, which is a dictionary with each token mapping to a list of document Ids for that token.\n","*  Calculate the Inverse Document Frequency (IDF) for each token by taking the log of the total number of documents divided by the document frequency of the token plus one and return the dictionary having the IDF values for each token."],"metadata":{"id":"If8DieyP1evc"}},{"cell_type":"code","source":["# Function to calculate inverse document frequency (IDF)\n","def calculate_idf(inverted_index, num_documents):\n","    idf = {}\n","    for token, doc_ids in inverted_index.items():\n","        idf[token] = math.log(num_documents / (len(doc_ids) + 1))\n","    return idf"],"metadata":{"id":"6o69KQipol7Q","executionInfo":{"status":"ok","timestamp":1690500408059,"user_tz":-60,"elapsed":237,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["*  Build the inverted index by processing the documents to obtain the tokens, and then, for each token, append the document Ids to the corresponding list for that token."],"metadata":{"id":"paEtlXZf28RW"}},{"cell_type":"code","source":["# Function to build inverted index\n","def build_inverted_index(documents):\n","    inverted_index = defaultdict(list)\n","    for doc_id, doc_content in documents.items():\n","        tokens = preprocess_document(doc_id, doc_content)\n","        for token in tokens:\n","            if doc_id not in inverted_index[token]:\n","                inverted_index[token].append(doc_id)\n","    return inverted_index"],"metadata":{"id":"LhlhxEmNoooK","executionInfo":{"status":"ok","timestamp":1690500418785,"user_tz":-60,"elapsed":244,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["*  Build a term-document matrix for using in the VSM model by iterating through each document, pre-processing its content and getting tokens, calculating the TF for each token, and finally computing the TF-IDF weight for each term in the document. Build it in the structure having the tokens as rows, document Ids as the columns, and TF-IDF weights as the values."],"metadata":{"id":"AKU5A5Eu4OzN"}},{"cell_type":"code","source":["# Function to build the term-document matrix for the Vector Space Model\n","def build_term_document_matrix(documents, inverted_index, idf):\n","    term_document_matrix = defaultdict(dict)\n","    for doc_id, doc_content in documents.items():\n","        tokens = preprocess_document(doc_id, doc_content)\n","        tf = calculate_tf(tokens)\n","        for token, freq in tf.items():\n","            tfidf = freq * idf[token]\n","            term_document_matrix[token][doc_id] = tfidf\n","    return term_document_matrix"],"metadata":{"id":"Rh2MQSbkorSk","executionInfo":{"status":"ok","timestamp":1690500423384,"user_tz":-60,"elapsed":294,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["*  Calculate BM25 score between a given query and a document using the BM25 ranking formula. It takes into account term frequencies and document frequencies and iterates through each token in the query, calculates the term's TF and IDF in the document, and then calculates the BM25 score based on these values. The final score is the sum of the scores for all tokens in the query."],"metadata":{"id":"QtKGeEpH5nHX"}},{"cell_type":"code","source":["# Function to calculate the BM25 score for a query and a document\n","def calculate_bm25_score(query_tokens, doc_tokens, inverted_index, num_documents, avg_doc_length, k1=1.5, b=0.75):\n","    score = 0\n","    doc_len = len(doc_tokens)\n","    for token in query_tokens:\n","        if token in inverted_index:\n","            df = len(inverted_index[token])\n","            idf = math.log((num_documents - df + 0.5) / (df + 0.5) + 1.0)\n","            tf = doc_tokens.count(token)\n","            numerator = tf * (k1 + 1)\n","            denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_length))\n","            score += idf * (numerator / denominator)\n","    return score"],"metadata":{"id":"HzUOvmtqby5l","executionInfo":{"status":"ok","timestamp":1690500430381,"user_tz":-60,"elapsed":278,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["*  Calculate the BM25 Language Model score between a given query and a document by iterating through each token in the query and computing the token's probability in the document based on the document's language model and the collection's language model. The final score is the product of the probabilities for all tokens in the query."],"metadata":{"id":"0sAKmo706WEx"}},{"cell_type":"code","source":["# Function to calculate the BM25 Language Model score for a query and a document\n","def calculate_language_model_score(query_tokens, doc_tokens, token_freq, total_token_count, lambd=0.5):\n","    score = 1\n","    doc_len = len(doc_tokens)\n","    for token in query_tokens:\n","        tf = doc_tokens.count(token)\n","        prob = (1 - lambd) * (tf / doc_len) + lambd * (token_freq[token] / total_token_count)\n","        score *= prob\n","    return score"],"metadata":{"id":"yeiGiwfUbyrh","executionInfo":{"status":"ok","timestamp":1690500438400,"user_tz":-60,"elapsed":183,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["*  Rank documents using the VSM model by calculating the VSM score for each document in the term-document matrix based on the query tokens. The scores are accumulated for each document, then sorted in descending order using their scores, to get the ranked list of documents."],"metadata":{"id":"TxeowMGv635r"}},{"cell_type":"code","source":["# Function to rank documents using the Vector Space Model\n","def rank_documents_vsm(query_tokens, term_document_matrix):\n","    scores = defaultdict(float)\n","    for token in query_tokens:\n","        if token in term_document_matrix:\n","            for doc_id, tfidf in term_document_matrix[token].items():\n","                scores[doc_id] += tfidf\n","    ranked_docs = sorted(scores.items(), key=itemgetter(1), reverse=True)\n","    return ranked_docs"],"metadata":{"id":"5R9pcI8VbyfX","executionInfo":{"status":"ok","timestamp":1690500444648,"user_tz":-60,"elapsed":278,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["*  Same as the previous one, but rank documents using the BM25 model by calculating the BM25 score for each document in the term-document matrix based on the query tokens. The scores are accumulated for each document, then sorted in descending order using their scores, to get the ranked list of documents."],"metadata":{"id":"Qt8HlNAz7uP1"}},{"cell_type":"code","source":["# Function to rank documents using BM25\n","def rank_documents_bm25(query_tokens, documents, inverted_index, num_documents, avg_doc_length):\n","    scores = defaultdict(float)\n","    for doc_id, doc_content in documents.items():\n","        doc_tokens = preprocess_document(doc_id, doc_content)\n","        score = calculate_bm25_score(query_tokens, doc_tokens, inverted_index, num_documents, avg_doc_length)\n","        scores[doc_id] = score\n","    ranked_docs = sorted(scores.items(), key=itemgetter(1), reverse=True)\n","    return ranked_docs"],"metadata":{"id":"liZXLpENo0VT","executionInfo":{"status":"ok","timestamp":1690500450871,"user_tz":-60,"elapsed":231,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["*  Same like previous ones, rank documents using the LM model by calculating the LM score for each document in the term-document matrix based on the query tokens. The scores are accumulated for each document, then sorted in descending order using their scores, to get the ranked list of documents."],"metadata":{"id":"6Sh9cbH48IEq"}},{"cell_type":"code","source":["# Function to rank documents using Language Model (LM)\n","def rank_documents_language_model(query_tokens, documents, token_freq, total_token_count):\n","    scores = defaultdict(float)\n","    for doc_id, doc_content in documents.items():\n","        doc_tokens = preprocess_document(doc_id, doc_content)\n","        score = calculate_language_model_score(query_tokens, doc_tokens, token_freq, total_token_count)\n","        scores[doc_id] = score\n","    ranked_docs = sorted(scores.items(), key=itemgetter(1), reverse=True)\n","    return ranked_docs"],"metadata":{"id":"XDBpu36so27n","executionInfo":{"status":"ok","timestamp":1690500456918,"user_tz":-60,"elapsed":318,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["*  Write the ranked documents in the specific format to an output file which will be used later on to evaluate using pytrec_eval to compute evaluation metrics for MAP, P@5, and NDCG. It creates an output file for each indexing models (SVM, BM25, and LM) having the ranked documents and their corresponding similarity scores to use later for the evaluations."],"metadata":{"id":"DmMtC__M8sM7"}},{"cell_type":"code","source":["# Function to write the output file in the required format\n","def write_output_file(query_id, ranked_docs, model_name, results_dir):\n","    # Create a new folder \"results\" if it doesn't exist\n","    if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","\n","    output_file = os.path.join(results_dir, f'{model_name}_output.txt')\n","    with open(output_file, 'a') as f:\n","        for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n","            # As the values of iter, rank, and run_id are irrelevant for trec_eval, we can set them to fixed values.\n","            # For example, we can set iter and run_id to 0, and rank to the rank of the document in the ranked list.\n","            iter_value = 0\n","            rank_value = rank\n","            similarity_value = score\n","            run_id_value = \"IR_System\"\n","            f.write(f\"{query_id} {iter_value} {doc_id} {rank_value} {similarity_value:.6f} {run_id_value}\\n\")"],"metadata":{"id":"kNP5LVSNbyS0","executionInfo":{"status":"ok","timestamp":1690500462846,"user_tz":-60,"elapsed":201,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["*  Read the Cranfield relevance file (cranqrel), extract the relevance judgments, and store them in a nested dictionary, which will make the relevance scores for specific queries and documents easily available at the time of the evaluation process."],"metadata":{"id":"CQbtVx11967G"}},{"cell_type":"code","source":["# Load the relevance judgments\n","def load_relevance_judgments(relevance_file):\n","    relevance_judgments = defaultdict(dict)\n","    with open(relevance_file, 'r', encoding='utf-8') as f_qrel:\n","        for line in f_qrel:\n","            parts = line.strip().split()\n","            if len(parts) != 3:\n","                # Skip lines with incorrect format\n","                continue\n","            query_id, object_id, relevance = parts\n","            try:\n","                relevance_judgments[query_id][object_id] = int(relevance)\n","            except ValueError:\n","                # Skip lines with incorrect relevance values\n","                continue\n","    return relevance_judgments"],"metadata":{"id":"up2tvXMs2Am2","executionInfo":{"status":"ok","timestamp":1690500469812,"user_tz":-60,"elapsed":293,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["*  This is a custom implementation of pytrec_eval::parse_run to handle multiple document Ids for the same query and allow the evaluation process to proceed without the AssertionError. Using a nested defaultdict so that if a document Id already exists for a specific query, then the new score will overwrite the old one."],"metadata":{"id":"jRLnw3pa_nfN"}},{"cell_type":"code","source":["# Custom implementation of parse_run of pytrec_eval to handle multiple document Ids for the same query and proceed without any error\n","def parse_run(f_run):\n","    run = defaultdict(dict)\n","    for line in f_run:\n","        query_id, _, object_id, _, score, _ = line.strip().split()\n","        run[query_id][object_id] = float(score)\n","    return run"],"metadata":{"id":"zaSOWZuf-SCv","executionInfo":{"status":"ok","timestamp":1690500476125,"user_tz":-60,"elapsed":172,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["*  For each model, read the output file having the retrieved documents and their scores.\n","*  Perform the evaluation of the retrieval models (VSM, BM25, LM) using the pytrec_eval library using the the relevance judgments and the evaluation measures (MAP, P@5, and NDCG).\n","*  Process and return the evaluation scores for each model in a dictionary where the model name is the key and the evaluation scores is the value."],"metadata":{"id":"Q7Mgo_-B_Nku"}},{"cell_type":"code","source":["# Function to evaluate retrieval models using pytrec_eval\n","def evaluate_models(model_names, output_dir, relevance_judgments):\n","    evaluation_results = {}\n","    for model_name in model_names:\n","        output_file = os.path.join(output_dir, f'{model_name}_output.txt')\n","        with open(output_file, 'r') as f:\n","          # using the custom parse_run to handle duplicate document Id for the same query scenarios\n","          run = parse_run(f)\n","          #run = pytrec_eval.parse_run(f)\n","\n","        # Using MAP, P@5, and NDCG measures for the evaluation\n","        evaluator = pytrec_eval.RelevanceEvaluator(relevance_judgments, {'map', 'P_5', 'ndcg'})\n","        model_scores = evaluator.evaluate(run)\n","        #print(model_scores.values())\n","        # Store the evaluation results in a dictionary\n","        evaluation_results[model_name] = model_scores\n","\n","    return evaluation_results"],"metadata":{"id":"sjzf4c9EwK3X","executionInfo":{"status":"ok","timestamp":1690500482137,"user_tz":-60,"elapsed":180,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":[" **THE MAIN FUNCTION:**\n","1. Load the Cranfield collection from the collection file - multiple documents each with a unique document Id and content.\n","\n","2. Preprocess the documents - tokenize, remove stop words, and apply stemming. Build an inverted index which maps each token to the list of document Ids containing that token.\n","\n","3. Calculate the IDF - for each token use the corresponding inverted index and total number of documents.\n","\n","4. Load the queries from the cranqrel file - each has a unique query Id and content.\n","\n","5. Build the term-document matrix for the VSM model using the preprocessed documents, inverted index, and IDF values.\n","\n","6. Rank the documents for each query using the VSM model and write the top 100 ranked documents to an output file with the prefix 'vsm'.\n","\n","7. Rank the documents for each query using the BM25 model, and write the top 100 ranked documents to an output file with the prefix 'bm25'.\n","\n","8. Rank the documents for each query using the LM model (Okapi BM25 Language Model), and write the top 100 ranked documents to an output file with the prefix 'bm25_lm'.\n","\n","9. Load the relevance judgments for queries and documents from the cranqrel file.\n","\n","10. Evaluate the retrieval models (VSM, BM25, and LM) using pytrec_eval based on the relevance judgments and the retrieved document rankings.\n","\n","11. Convert the nested dictionary of evaluation results into a DataFrame, rename the columns in more understandable way, and set the decimal places, for easier analysis and manipulation.\n","\n","12. Save the evaluation results DataFrame as a CSV file, containing Mean Average Precision (MAP), Precision at 5 (P@5), and Normalized Discounted Cumulative Gain (NDCG), for each retrieval model and query for the top 100 documents."],"metadata":{"id":"sNwBkxIqBY8c"}},{"cell_type":"code","source":["# The MAIN Function to run the IR System and Save & Print the evaluation results\n","if __name__ == \"__main__\":\n","    # Load the Cranfield collection\n","    documents = load_cranfield_collection(collection_file)\n","\n","    # Preprocess the documents and build the inverted index\n","    inverted_index = build_inverted_index(documents)\n","\n","    # Calculate inverse document frequency (IDF)\n","    num_documents = len(documents)\n","    idf = calculate_idf(inverted_index, num_documents)\n","\n","    # Load the queries\n","    queries = load_queries(queries_file)\n","\n","    # Build the term-document matrix for Vector Space Model\n","    term_document_matrix = build_term_document_matrix(documents, inverted_index, idf)\n","\n","    # Rank documents using Vector Space Model\n","    for query_id, query_content in queries.items():\n","        query_tokens = preprocess_document(query_id, query_content)\n","        ranked_docs_vsm = rank_documents_vsm(query_tokens, term_document_matrix)\n","        write_output_file(query_id, ranked_docs_vsm[:100], 'vsm', output_dir)\n","\n","    # Rank documents using BM25\n","    avg_doc_length = sum(len(preprocess_document(doc_id, doc_content)) for doc_id, doc_content in documents.items()) / num_documents\n","    for query_id, query_content in queries.items():\n","        query_tokens = preprocess_document(query_id, query_content)\n","        ranked_docs_bm25 = rank_documents_bm25(query_tokens, documents, inverted_index, num_documents, avg_doc_length)\n","        write_output_file(query_id, ranked_docs_bm25[:100], 'bm25', output_dir)\n","\n","    # Rank documents using Okapi BM25 Language Model\n","    token_freq = defaultdict(int)\n","    total_token_count = 0\n","    for doc_id, doc_content in documents.items():\n","        doc_tokens = preprocess_document(doc_id, doc_content)\n","        for token in doc_tokens:\n","            token_freq[token] += 1\n","            total_token_count += 1\n","    for query_id, query_content in queries.items():\n","        query_tokens = preprocess_document(query_id, query_content)\n","        ranked_docs_lm = rank_documents_language_model(query_tokens, documents, token_freq, total_token_count)\n","        write_output_file(query_id, ranked_docs_lm[:100], 'bm25_lm', output_dir)\n","\n","    # Load the relevance judgments from the cranqrel file\n","    relevance_judgments = load_relevance_judgments(relevance_file)\n","    #print(relevance_judgments.values())\n","\n","    # Evaluate retrieval models using pytrec_eval\n","    model_names = ['vsm', 'bm25', 'bm25_lm']\n","    evaluation_results = evaluate_models(model_names, output_dir, relevance_judgments)\n","\n","    # Print the evaluation results\n","    print(\"\\033[1m\\033[4mEvaluation Results:\\033[0m\")\n","\n","    # Convert the nested dictionary into a DataFrame\n","    data = {}\n","    for model, model_results in evaluation_results.items():\n","        for query, query_results in model_results.items():\n","            if all(key in query_results for key in ['map', 'P_5', 'ndcg']):\n","                data[(model, query)] = query_results\n","\n","    df = pd.DataFrame.from_dict(data, orient='index')\n","    # Rename the columns and index (row) with custom names\n","    df.columns = ['MAP_Score', 'P@5_Score', 'NDCG_Score']\n","    df.index.name = 'Model'\n","\n","    # Set the number of decimal places\n","    df = df.round(4)\n","    # Transpose the DataFrame to get a more readable format\n","    #df = df.T\n","\n","    # Save the Evaluation Results to a file\n","    df.to_csv(output_dir + 'evaluation_results.csv')\n","    print(df)\n","\n","    # Load the relevance judgments from the cranqrel file\n","    relevance_judgments = load_relevance_judgments(relevance_file)\n","    #print(relevance_judgments.values())\n","\n","    # Evaluate retrieval models using pytrec_eval\n","    model_names = ['vsm', 'bm25', 'bm25_lm']\n","    evaluation_results = evaluate_models(model_names, output_dir, relevance_judgments)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdEx4AmBdK-j","executionInfo":{"status":"ok","timestamp":1690500585114,"user_tz":-60,"elapsed":2000,"user":{"displayName":"Pradip Mallick","userId":"06111683699714911533"}},"outputId":"efad5d8b-691e-4774-8702-418c8447ccec"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m\u001b[4mEvaluation Results:\u001b[0m\n","             MAP_Score  P@5_Score  NDCG_Score\n","vsm     100     0.0022        0.0      0.0462\n","        101     0.0000        0.0      0.0000\n","        102     0.0000        0.0      0.0000\n","        103     0.0000        0.0      0.0000\n","        104     0.0000        0.0      0.0000\n","...                ...        ...         ...\n","bm25_lm 218     0.0000        0.0      0.0000\n","        219     0.0000        0.0      0.0000\n","        223     0.0000        0.0      0.0000\n","        224     0.0000        0.0      0.0000\n","        225     0.0186        0.0      0.1484\n","\n","[282 rows x 3 columns]\n"]}]}]}